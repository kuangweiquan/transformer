# Transformer模型详解

## 引言
Transformer模型由Google团队在2017年发表的论文《Attention is All You Need》中提出，彻底改变了自然语言处理领域。它摒弃了传统的循环神经网络(RNN)和卷积神经网络(CNN)，完全基于注意力机制构建，显著提高了序列建模的效率和质量。本教程将深入解析Transformer的核心原理。

![Transformer架构](https://miro.medium.com/max/1200/1*BHzGVskWGS_3jEcYYi6miQ.png)

## 目录
1. Transformer核心组件
2. 自注意力机制详解
3. 位置编码原理
4. 编码器与解码器结构
---

## 1. Transformer核心组件

### 1.1 自注意力机制(Self-Attention)
自注意力机制允许序列中的每个位置计算与其他位置的关联程度，从而捕获全局依赖关系。其核心思想是：对于序列中的每个元素（例如一个单词），计算它与序列中所有元素的相关性得分，然后根据这些得分加权求和得到该元素的新的表示。

**计算步骤**：
1. 将输入向量分别与三个权重矩阵相乘，得到查询向量(Query)、键向量(Key)和值向量(Value)
2. 计算每个Query与所有Key的点积，得到注意力分数
3. 对注意力分数进行缩放和Softmax归一化
4. 用归一化后的分数对Value向量加权求和

**计算公式**：
![自注意力公式](https://latex.codecogs.com/svg.latex?%5Cdpi%7B150%7D%20%5Cbg_white%20%5Clarge%20%5Ctext%7BAttention%7D(Q%2CK%2CV)%20%3D%20%5Ctext%7Bsoftmax%7D%5Cleft(%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%5Cright)V)

其中：
- $Q$: 查询矩阵 (Query)
- $K$: 键矩阵 (Key)
- $V$: 值矩阵 (Value)
- $d_k$: 键向量的维度（缩放因子防止点积过大）

### 1.2 多头注意力(Multi-Head Attention)
多头注意力将自注意力机制并行执行多次，增强模型捕获不同表示子空间信息的能力：

1. 将输入向量分割到多个"头"中
2. 每个头独立进行自注意力计算
3. 将所有头的输出拼接起来
4. 通过线性层融合结果

**公式表示**：
$$
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
$$

每个注意力头：
$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

### 1.3 位置前馈网络(Position-wise Feed Forward)
位置前馈网络是一个两层的全连接网络，对序列中的每个位置独立进行相同的非线性变换：

$$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$

### 1.4 层归一化(Layer Normalization)与残差连接(Residual Connection)
Transformer使用层归一化和残差连接解决深度网络训练难题：

**残差连接**：将子层的输入直接加到子层的输出上
**层归一化**：对每个样本的所有特征进行归一化

**计算过程**：
$$
\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))
$$

---

## 2. 自注意力机制深入解析

### 2.1 为什么需要自注意力？
传统RNN在处理长序列时存在梯度消失问题和顺序计算的限制。自注意力机制的优势在于：
- 能够直接计算序列中任意两个元素的关系
- 支持完全并行计算
- 对长距离依赖关系建模能力强

### 2.2 缩放点积注意力的数学原理
点积$QK^T$计算了查询和键向量的相似度。缩放因子$\sqrt{d_k}$防止当$d_k$较大时，点积结果过大导致Softmax梯度消失。

Softmax归一化：
$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$
将注意力分数转化为概率分布，确保所有位置的权重和为1。

### 2.3 多头注意力的设计哲学
多头机制允许模型：
1. 在不同表示子空间学习不同关系
2. 并行捕获语法、语义等不同层面的信息
3. 增强模型的表达能力和泛化能力

实验表明8个头效果最佳，每个头的维度为总维度除以头数。

---

## 3. 位置编码(Positional Encoding)

### 3.1 为什么需要位置编码？
自注意力机制本身不包含序列顺序信息，因此需要显式添加位置信息。位置编码与输入向量具有相同维度，可以直接相加。

### 3.2 正弦/余弦位置编码
Transformer使用不同频率的正弦和余弦函数生成位置编码：

$$
PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

$$
PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

其中：
- $pos$: 位置索引
- $i$: 维度索引
- $d_{\text{model}}$: 模型维度

### 3.3 位置编码的特性
1. **唯一性**：每个位置有唯一编码
2. **相对位置可学习**：位置$pos+k$的编码可以表示为$pos$的线性函数
3. **值域有界**：所有值在[-1,1]范围内
4. **可扩展到长序列**：支持训练中未见过的序列长度

---

## 4. 编码器与解码器结构

### 4.1 编码器(Encoder)
![编码器结构](https://jalammar.github.io/images/t/transformer_encoder_decoder_stack.png)

编码器由N个相同层堆叠而成（原论文N=6），每层包含：
1. **多头自注意力层**：捕获输入序列内部依赖
2. **位置前馈网络**：非线性变换
3. **残差连接+层归一化**：每个子层后应用

输入序列经过嵌入层和位置编码后输入编码器，最终输出上下文感知的表示。

### 4.2 解码器(Decoder)
解码器同样由N个相同层堆叠，每层包含三个子层：
1. **带掩码的多头自注意力层**：防止当前位置关注未来信息
2. **编码器-解码器注意力层**：连接编码器输出与当前解码状态
3. **位置前馈网络**：非线性变换

### 4.3 关键区别：掩码自注意力
解码器第一子层使用掩码自注意力：
$$
\text{MaskedAttention}(Q,K,V) = \text{softmax}\left(\frac{QK^T + M}{\sqrt{d_k}}\right)V
$$
其中M是掩码矩阵，上三角元素设为$-\infty$（实际实现中设为极大负数），确保当前位置只能关注之前位置。

### 4.4 编码器-解码器注意力
这一层使用解码器输出作为Query，编码器输出作为Key和Value：
- Query：当前解码状态
- Key：编码器输出的上下文表示
- Value：编码器输出的上下文表示

这使得解码器能够在生成每个token时关注输入序列中最相关的部分。

---

## 结论
Transformer模型凭借其强大的注意力机制和并行计算能力，已成为现代深度学习的基础架构。从原始Transformer到BERT、GPT等衍生模型，其核心思想持续推动NLP、CV和多模态领域的发展。理解自注意力机制、位置编码和编码器-解码器结构是掌握现代深度学习模型的关键。
